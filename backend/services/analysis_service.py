# Code Generated by Sidekick is for learning and experimentation purposes only.
import os
import zipfile
import re
import json
from openai import AsyncOpenAI
from tortoise import Tortoise
from models.projects import Project, FileMetadata, ProjectStatus, CodeChunk
from langchain_text_splitters import Language, RecursiveCharacterTextSplitter
from dotenv import load_dotenv
from collections import Counter
from tortoise.transactions import in_transaction

import asyncio
import json
from typing import List
from fastapi import HTTPException
from tortoise import Tortoise

from services.progress_manager import ProgressManager, ProgressManager, get_progress_manager
from .metadata_extractor import extract_metadata_by_extension

load_dotenv(override=True)

# --- Configuration & Constants ---
IGNORE_DIRS = {'.git', 'node_modules', '__pycache__', 'venv', '.venv', 'dist', 'build', 'target', 'bin', 'obj'}
ENTRY_POINTS = {'main.py', 'app.py', 'index.js', 'server.js', 'src/main.ts', 'main.go', 'Cargo.toml', 'pom.xml'}

# Mapping config files to Languages
CONFIG_MAP = {
    'package.json': 'JavaScript/TypeScript',
    'requirements.txt': 'Python',
    'pyproject.toml': 'Python',
    'pom.xml': 'Java',
    'build.gradle': 'Java',
    'composer.json': 'PHP',
    'go.mod': 'Go',
    'Cargo.toml': 'Rust',
    'Gemfile': 'Ruby',
    'mix.exs': 'Elixir',
    'pubspec.yaml': 'Dart',
    'Dockerfile': 'Docker',
}

# Mapping extensions to Languages
EXTENSION_TO_LANGUAGE = {
    '.py': 'Python',
    '.js': 'JavaScript',
    '.ts': 'TypeScript',
    '.tsx': 'TypeScript',
    '.jsx': 'JavaScript',
    '.java': 'Java',
    '.go': 'Go',
    '.rs': 'Rust',
    '.rb': 'Ruby',
    '.php': 'PHP',
    '.cs': 'C#',
    '.cpp': 'C++',
    '.c': 'C',
    '.html': 'HTML',
    '.css': 'CSS',
    '.sql': 'SQL',
}

# Mapping extensions to LangChain Splitters
EXTENSION_TO_LANG = {
    '.py': Language.PYTHON,
    '.js': Language.JS,
    '.ts': Language.TS,
    '.tsx': Language.TS,
    '.jsx': Language.JS,
    '.java': Language.JAVA,
    '.go': Language.GO,
    '.rs': Language.RUST,
    '.rb': Language.RUBY,
    '.php': Language.PHP,
    '.cs': Language.CSHARP,
    '.cpp': Language.CPP,
    '.c': Language.C,
    '.html': Language.HTML,
}

client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# --- Intelligence Helpers ---

def detect_framework_and_endpoints(file_contents: str, extension: str):
    """
    Scans code for framework markers and API endpoints across various languages.
    Returns: (framework_name, endpoints_list)
    """
    framework = None
    endpoints = []
    content_lower = file_contents.lower()

    # 1. Python Detection
    if extension == ".py":
        if "fastapi" in content_lower:
            framework = "FastAPI"
            endpoints = re.findall(r'@app\.(get|post|put|delete|patch)\(["\'](.+?)["\']\)', file_contents)
        elif "flask" in content_lower:
            framework = "Flask"
            endpoints = re.findall(r'@app\.route\(["\'](.+?)["\']', file_contents)
        elif "django" in content_lower:
            framework = "Django"

    # 2. JS / TS Detection
    elif extension in [".js", ".ts", ".tsx", ".jsx"]:
        if "express" in content_lower:
            framework = "Express"
            endpoints = re.findall(r'app\.(get|post|put|delete)\(["\'](.+?)["\']', file_contents)
        elif "@nestjs" in content_lower or "nest" in content_lower:
            framework = "NestJS"
            endpoints = re.findall(r'@(Get|Post|Put|Delete)\(["\'](.+?)["\']\)', file_contents)
        elif "next" in content_lower or "next/app" in content_lower:
            framework = "Next.js"
        elif "react" in content_lower:
            framework = "React"
        elif "vue" in content_lower:
            framework = "Vue.js"
        elif "angular" in content_lower:
            framework = "Angular"

    # 3. Java Detection
    elif extension == ".java":
        if "@springbootapplication" in content_lower or "@restcontroller" in content_lower:
            framework = "Spring Boot"
            endpoints = re.findall(r'@(Get|Post|Put|Delete)Mapping\(["\'](.+?)["\']\)', file_contents)

    # 4. Go Detection
    elif extension == ".go":
        if "gin" in content_lower:
            framework = "Gin"
            endpoints = re.findall(r'\.(GET|POST|PUT|DELETE)\(["\'](.+?)["\']', file_contents)
        elif "fiber" in content_lower:
            framework = "Fiber"
        elif "echo" in content_lower:
            framework = "Echo"

    # 5. PHP Detection
    elif extension == ".php":
        if "laravel" in content_lower or "route::" in content_lower:
            framework = "Laravel"
            endpoints = re.findall(r'Route::(get|post|put|delete)\(["\'](.+?)["\']', file_contents)
        elif "symfony" in content_lower:
            framework = "Symfony"

    # 6. Ruby Detection
    elif extension == ".rb":
        if "rails" in content_lower or "actioncontroller" in content_lower:
            framework = "Ruby on Rails"
        elif "sinatra" in content_lower:
            framework = "Sinatra"
            endpoints = re.findall(r'(get|post|put|delete)\s+["\'](.+?)["\']\s+do', file_contents)

    # 7. C# (.NET) Detection
    elif extension == ".cs":
        if "microsoft.aspnetcore" in content_lower or "webapplication" in content_lower:
            framework = "ASP.NET Core"
            endpoints = re.findall(r'\[(HttpGet|HttpPost|HttpPut|HttpDelete)\(["\'](.+?)["\']\)\]', file_contents)

    # 8. Rust Detection
    elif extension == ".rs":
        if "actix" in content_lower:
            framework = "Actix Web"
        elif "rocket" in content_lower:
            framework = "Rocket"

    return framework, endpoints


def identify_chunk_type(text: str, extension: str):
    """
    Language-agnostic logic type identification.
    """
    stripped = text.strip()
    if re.match(r'^(class|public class|private class|struct|enum)\s+\w+', stripped):
        return "Class/Structure"
    if re.match(r'^(def|func|function|public|private|async function)\s+\w+', stripped):
        return "Function/Method"
    if "=>" in stripped or "->" in stripped:
        return "Arrow Function/Lambda"
    return "Code Block"


def detect_api_routes(content: str, extension: str) -> bool:
    """
    Detect if file contains API route definitions
    """
    if extension == ".py":
        patterns = [
            r'@(?:app|router)\.(get|post|put|delete|patch)',
            r'@app\.route',
            r'class.*\((?:.*APIView.*)\)',
        ]
        return any(re.search(pattern, content, re.IGNORECASE) for pattern in patterns)
    
    elif extension in [".js", ".ts", ".jsx", ".tsx"]:
        patterns = [
            r'(?:app|router)\.(get|post|put|delete|patch)\(',
            r'@(Get|Post|Put|Delete|Patch)\(',
            r'express\(\)',
        ]
        return any(re.search(pattern, content, re.IGNORECASE) for pattern in patterns)
    
    elif extension == ".java":
        patterns = [
            r'@(GetMapping|PostMapping|PutMapping|DeleteMapping|RequestMapping)',
            r'@RestController',
        ]
        return any(re.search(pattern, content) for pattern in patterns)
    
    elif extension == ".go":
        patterns = [
            r'\.(GET|POST|PUT|DELETE)\(',
            r'http\.HandleFunc',
        ]
        return any(re.search(pattern, content) for pattern in patterns)
    return False


def detect_db_models(content: str, extension: str) -> bool:
    """ Detect if file contains database model definitions """
    if extension == ".py":
        patterns = [
            r'class\s+\w+\(.*Model.*\)',
            r'class\s+\w+\(.*BaseModel.*\)',
            r'from.*models import',
            r'db\.Model',
            r'Column\(',
        ]
        return any(re.search(pattern, content, re.IGNORECASE) for pattern in patterns)
    elif extension in [".js", ".ts", ".jsx", ".tsx"]:
        patterns = [
            r'new Schema\(',
            r'@Entity\(',
            r'sequelize\.define',
            r'@Column\(',
        ]
        return any(re.search(pattern, content, re.IGNORECASE) for pattern in patterns)
    elif extension == ".java":
        patterns = [
            r'@Entity',
            r'@Table',
            r'@Column',
        ]
        return any(re.search(pattern, content) for pattern in patterns)
    elif extension == ".go":
        return 'gorm.Model' in content
    return False


def detect_auth_code(content: str) -> bool:
    auth_keywords = [
        'authenticate', 'authorization', 'jwt', 'token', 'login', 'logout',
        'password', 'hash', 'bcrypt', 'oauth', 'session', 'credentials',
        'auth_user', 'current_user', 'verify_token', 'get_current_user'
    ]
    content_lower = content.lower()
    return any(keyword in content_lower for keyword in auth_keywords)


def is_config_file(relative_path: str) -> bool:
    config_files = [
        'config.py', 'settings.py', 'configuration.py', 'config.js', 'config.ts',
        '.env', '.env.example', 'application.properties', 'application.yml',
        'appsettings.json', 'web.config', 'package.json', 'pyproject.toml',
        'requirements.txt', 'Cargo.toml', 'go.mod', 'pom.xml', 'build.gradle'
    ]
    filename = os.path.basename(relative_path).lower()
    return filename in [f.lower() for f in config_files]


def is_entry_point(relative_path: str, content: str) -> bool:
    filename = os.path.basename(relative_path)
    entry_files = [
        'main.py', 'app.py', '__main__.py', 'run.py', 'index.js', 'server.js',
        'app.js', 'main.js', 'index.ts', 'server.ts', 'app.ts', 'main.ts',
        'Main.java', 'Application.java', 'main.go', 'main.rs',
    ]
    if filename in entry_files:
        return True
    entry_patterns = [
        r'if __name__ == ["\']__main__["\']',
        r'app\.listen\(',
        r'public static void main\(',
        r'func main\(',
        r'fn main\(',
    ]
    return any(re.search(pattern, content) for pattern in entry_patterns)


def count_endpoints(content: str, extension: str) -> int:
    count = 0
    if extension == ".py":
        patterns = [r'@(?:app|router)\.(get|post|put|delete|patch)\(', r'@app\.route\(']
        for pattern in patterns:
            count += len(re.findall(pattern, content, re.IGNORECASE))
    elif extension in [".js", ".ts", ".jsx", ".tsx"]:
        patterns = [r'(?:app|router)\.(get|post|put|delete|patch)\(', r'@(Get|Post|Put|Delete|Patch)\(']
        for pattern in patterns:
            count += len(re.findall(pattern, content, re.IGNORECASE))
    elif extension == ".java":
        patterns = [r'@(GetMapping|PostMapping|PutMapping|DeleteMapping)']
        for pattern in patterns:
            count += len(re.findall(pattern, content))
    return count


def extract_imports(content: str, extension: str) -> list:
    imports = []
    if extension == ".py":
        patterns = [r'from\s+([\w\.]+)\s+import', r'import\s+([\w\.]+)']
        for pattern in patterns:
            imports.extend(re.findall(pattern, content))
    elif extension in [".js", ".ts", ".jsx", ".tsx"]:
        patterns = [r'from\s+["\']([^"\']+)["\']', r'require\(["\']([^"\']+)["\']\)']
        for pattern in patterns:
            imports.extend(re.findall(pattern, content))
    elif extension == ".java":
        imports = re.findall(r'import\s+([\w\.]+);', content)
    elif extension == ".go":
        imports = re.findall(r'import\s+"([^"]+)"', content)
    return imports


def detect_language_from_config(extract_path: str) -> str:
    for config_file, language in CONFIG_MAP.items():
        if os.path.exists(os.path.join(extract_path, config_file)):
            return language
    return None

def is_binary_file(filepath: str) -> bool:
    try:
        with open(filepath, 'rb') as f:
            chunk = f.read(1024)
            return b'\0' in chunk
    except:
        return True

# --- Main Analysis Steps ---

async def preprocess_project(project: Project):
    progress = get_progress_manager(project)
    await progress.milestone("Starting preprocessing phase")
    extract_path = f"storage/extracted/{project.id}"
    
    try:
        
        zip_path = f"storage/uploads/{project.id}.zip"
        if not os.path.exists(zip_path):
            raise Exception(f"Upload file not found: {zip_path}")
        os.makedirs(extract_path, exist_ok=True)
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_path)
        await progress.processing("Project files extracted")
        
        detected_language = detect_language_from_config(extract_path)
        all_files = []
        skip_patterns = ['node_modules', '.git', '__pycache__', '.venv', 'venv', 'dist', 'build', '.next', 'target']
        
        for root, dirs, files in os.walk(extract_path):
            dirs[:] = [d for d in dirs if d not in skip_patterns]
            for file in files:
                file_path = os.path.join(root, file)
                relative_path = os.path.relpath(file_path, extract_path)
                all_files.append(relative_path)
        
        progress.set_total_steps(len(all_files))
        file_count = 0
        endpoint_count = 0
        extension_counter = Counter()
        framework_counter = Counter()
        
        for relative_path in all_files:
            full_path = os.path.join(extract_path, relative_path)
            try:
                if is_binary_file(full_path):
                    continue
                
                with open(full_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                
                extension = os.path.splitext(relative_path)[1]
                if extension in EXTENSION_TO_LANGUAGE:
                    extension_counter[extension] += 1
                
                framework, _ = detect_framework_and_endpoints(content, extension)
                if framework:
                    framework_counter[framework] += 1
                
                contains_api = detect_api_routes(content, extension)
                contains_db = detect_db_models(content, extension)
                contains_auth = detect_auth_code(content)
                
                if contains_api:
                    endpoint_count += count_endpoints(content, extension)
                
                await FileMetadata.create(
                    project=project,
                    file_path=relative_path,
                    file_name=os.path.basename(relative_path),
                    extension=extension,
                    size=os.path.getsize(full_path),
                    contains_api_routes=contains_api,
                    contains_db_models=contains_db,
                    contains_auth=contains_auth,
                    is_config_file=is_config_file(relative_path),
                    is_entry_point=is_entry_point(relative_path, content),
                    imports=extract_imports(content, extension)
                )
                file_count += 1
                await progress.processing(f"Analyzed: {os.path.basename(relative_path)}", increment=True)
            except Exception as e:
                await progress.warning(f"Error processing {relative_path}: {str(e)}")

        if not detected_language and extension_counter:
            most_common_ext = extension_counter.most_common(1)[0][0]
            detected_language = EXTENSION_TO_LANGUAGE.get(most_common_ext, "Unknown")
        
        project.file_count = file_count
        project.endpoint_count = endpoint_count
        project.detected_framework = framework_counter.most_common(1)[0][0] if framework_counter else "Not Detected"
        project.detected_language = detected_language or "Unknown"
        await project.save()
        await progress.milestone("Preprocessing complete")
    except Exception as e:
        await progress.error(f"Preprocessing failed: {str(e)}")
        raise


async def chunk_code(project_id: str, progress: ProgressManager):
    project = await Project.get(id=project_id)
    files = await FileMetadata.filter(project=project).all()
    extract_path = f"storage/extracted/{project.id}"
    
    progress.set_total_steps(len(files)) 

    for file_meta in files:
        full_path = os.path.join(extract_path, file_meta.file_path)
        if not os.path.exists(full_path): continue
        with open(full_path, "r", encoding="utf-8", errors="ignore") as f:
            code = f.read().replace("\x00", "")

        lang = EXTENSION_TO_LANG.get(file_meta.extension, Language.PYTHON)
        splitter = RecursiveCharacterTextSplitter.from_language(language=lang, chunk_size=1000, chunk_overlap=200)
        chunks = splitter.create_documents([code])

        for i, chunk in enumerate(chunks):
            await progress.processing(f"Chunking: {file_meta.file_name}", increment=True)
            await CodeChunk.create(
                project=project,
                file_metadata=file_meta,
                content=chunk.page_content,
                chunk_type=identify_chunk_type(chunk.page_content, file_meta.extension),
                start_line=i * 10,
                end_line=(i + 1) * 10
            )

async def generate_embeddings_for_project(project_id: str, progress: ProgressManager):
    chunks = await CodeChunk.filter(project_id=project_id).all()
    if not chunks: return
    batch_size = 50
    progress.set_total_steps(len(chunks))

    for i in range(0, len(chunks), batch_size):
        batch = chunks[i : i + batch_size]
        texts = [c.content for c in batch]
        try:
            response = await client.embeddings.create(input=texts, model="text-embedding-3-small")
        except Exception as e:
            print(f"OpenAI Error: {e}")
            continue

        await progress.processing(f"Embedded {min(i + batch_size, len(chunks))} chunks", increment=False)
        # Manually increment by batch size
        progress.current_step = min(i + batch_size, len(chunks))

        async with in_transaction("default") as conn:
            for j, chunk in enumerate(batch):
                embedding_str = json.dumps(response.data[j].embedding)
                await conn.execute_query('UPDATE "codechunk" SET "embedding" = $1::vector WHERE "id" = $2', [embedding_str, chunk.id])


# Reciprocal Rank Fusion constant (usually 60)
RRF_K = 60

async def generate_queries(original_query: str) -> List[str]:
    """Uses LLM to generate variations of the search query."""
    prompt = f"""You are a helpful assistant that generates multiple search queries based on a single input query. 
    Generate 5 alternative versions of the following code search query to help find relevant code snippets.
    Focus on: technical synonyms, different ways to describe the functionality, and specific library names if applicable.
    
    Original query: {original_query}
    
    Return only the queries, one per line, no numbering."""
    
    # Assuming you have an initialized openai client
    response = await client.chat.completions.create(
        model="gpt-4o-mini", # Or gpt-3.5-turbo
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2
    )
    
    queries = response.choices[0].message.content.strip().split("\n")
    # Clean up and ensure we include the original query
    queries = [q.strip() for q in queries if q.strip()]
    queries.append(original_query)
    return queries[:6] # Return max 6 variations

async def execute_single_search(project_id: str, embedding: List[float], limit: int):
    """Executes one vector search."""
    conn = Tortoise.get_connection("default")
    query_vector_str = json.dumps(embedding)
    
    sql = """
        SELECT c.content, c.chunk_type, f.file_path, (c.embedding <=> $1::vector) as distance
        FROM codechunk c JOIN filemetadata f ON c.file_metadata_id = f.id
        WHERE c.project_id = $2 
        ORDER BY distance ASC 
        LIMIT $3
    """
    return await conn.execute_query_dict(sql, [query_vector_str, project_id, limit])

async def generate_answer(query: str, contexts: List[Dict[str, Any]]) -> str:
    """Generates a technical answer based on the top 20 retrieved snippets."""
    
    # Format the top 20 snippets as context for the LLM
    context_text = ""
    for i, doc in enumerate(contexts):
        context_text += f"---\nFILE: {doc['file_path']}\nTYPE: {doc['chunk_type']}\nCONTENT:\n{doc['content']}\n"

    system_prompt = """You are an expert software architect. Use the provided code snippets to answer the user's question. 
    - Be technical and precise. 
    - Reference specific file names and function names.
    - If the code doesn't contain the answer, say you don't know based on the context.
    - Use markdown for your response."""

    user_prompt = f"User Question: {query}\n\nRetrieved Context:\n{context_text}"

    response = await client.chat.completions.create(
        model="gpt-4o", # Use gpt-4o or gpt-4o-mini for better reasoning
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        temperature=0.1
    )
    return response.choices[0].message.content

async def semantic_code_search(project_id: str, query: str):
    # 1. Multi-Query Generation (as before)
    query_variations = await generate_queries(query)
    
    # 2. Batch Embedding
    res = await client.embeddings.create(input=query_variations, model="text-embedding-3-small")
    embeddings = [data.embedding for data in res.data]
    
    # 3. Parallel Search (fetch 30 per query to ensure diversity after RRF)
    search_tasks = [execute_single_search(project_id, emb, limit=30) for emb in embeddings]
    results_lists = await asyncio.gather(*search_tasks)
    
    # 4. RRF Re-ranking
    rrf_scores = {}
    metadata_map = {}
    
    for results in results_lists:
        for rank, doc in enumerate(results, start=1):
            doc_id = f"{doc['file_path']}:{hash(doc['content'])}" 
            if doc_id not in rrf_scores:
                rrf_scores[doc_id] = 0.0
                metadata_map[doc_id] = doc
            rrf_scores[doc_id] += 1.0 / (RRF_K + rank)
            
    sorted_doc_ids = sorted(rrf_scores.keys(), key=lambda x: rrf_scores[x], reverse=True)
    
    # Get top 20 for the LLM Context
    top_20_results = []
    for doc_id in sorted_doc_ids[:20]:
        doc = metadata_map[doc_id]
        doc['rrf_score'] = rrf_scores[doc_id]
        top_20_results.append(doc)

    # 5. Generate Answer based on Top 20
    # We do this before slicing to 5 so the LLM sees more context
    answer = await generate_answer(query, top_20_results)
    
    # 6. Return Answer + Top 5 Snippets
    return {
        "answer": answer,
        "files": top_20_results[:5] 
    }